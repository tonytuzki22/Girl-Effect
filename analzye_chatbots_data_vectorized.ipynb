{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "from nltk.classify.textcat import TextCat \n",
    "from matplotlib.pyplot import plot \n",
    "#from datascience import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give me more KERNEL SPACE!\n",
    "jupyter notebook --NotbookApp.iopub_Data_Rate_Limit=1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running servers:\r\n",
      "http://0.0.0.0:8888/user/emilyxu/ :: /home/jovyan\r\n"
     ]
    }
   ],
   "source": [
    "!jupyter notebook list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing default config to: /home/jovyan/.jupyter/jupyter_notebook_config.py\r\n"
     ]
    }
   ],
   "source": [
    "!jupyter notebook --generate-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter notebook --NotebookApp.max_buffer_size=10000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy[speedup] in /opt/conda/lib/python3.8/site-packages (0.18.0)\n",
      "Requirement already satisfied: python-levenshtein>=0.12; extra == \"speedup\" in /opt/conda/lib/python3.8/site-packages (from fuzzywuzzy[speedup]) (0.12.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from python-levenshtein>=0.12; extra == \"speedup\"->fuzzywuzzy[speedup]) (49.6.0.post20201009)\n",
      "Requirement already satisfied: python-Levenshtein in /opt/conda/lib/python3.8/site-packages (0.12.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from python-Levenshtein) (49.6.0.post20201009)\n"
     ]
    }
   ],
   "source": [
    "#fuzzywuzzy\n",
    "!pip3 install fuzzywuzzy[speedup]\n",
    "!pip install python-Levenshtein\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "#(py)torch\n",
    "#!pip install torch\n",
    "\n",
    "#langid (language identification)\n",
    "#!pip install langid\n",
    "#import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#DO NOT RUN \n",
    "\n",
    "chatbots_data = Table.read_table(\"chatbots_data.csv\")\n",
    "\n",
    "#Convert Pandas dataframe to Table (where you can use table.select(...)). Not suggested -- takes up lots of memory\n",
    "chatbots_data = Table.from_df(chatbots_data_chunks, keep_index=False)\n",
    "chatbots_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 7)\n",
      "==================================================================\n",
      "              Created On     Flow Name  Merged Code  \\\n",
      "0  5/27/2020 10:31:06 PM    OB_NewUser          NaN   \n",
      "1   5/27/2020 7:38:52 PM  HLP_Feedback          NaN   \n",
      "\n",
      "                                             Message    Org Name  \\\n",
      "0                                            Kya kru   Chhaa Jaa   \n",
      "1  I like you\\nAnd I think improve not only about...  Big Sis V3   \n",
      "\n",
      "   Referred Flow                                  Uuid  \n",
      "0            NaN  915617a9-c0ae-47fc-8df0-54d72d4f41d9  \n",
      "1            NaN  148d099c-54eb-4182-961c-068eeda96691  \n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "#dividing the data into chunks of 10, because of memory issues (kernel keeps dying)\n",
    "#for example, this is what one chunk looks like\n",
    "chunksize = 10\n",
    "for chunk in pd.read_csv(\"chatbots_data.csv\", chunksize = chunksize):\n",
    "    print(chunk.shape)\n",
    "    print(\"=\"*66)\n",
    "    print(chunk.head(2))\n",
    "    print(\"=\"*66)\n",
    "    break\n",
    "    \n",
    "#appending chunks to chatbot_list, an empty array\n",
    "chatbot_list = []\n",
    "chunksize = 10\n",
    "for chunk in pd.read_csv(\"chatbots_data.csv\", chunksize = chunksize):\n",
    "    chatbot_list.append(chunk)\n",
    "\n",
    "#Combine all the chunks into one dataframe\n",
    "chatbots_data_chunks = pd.concat(chatbot_list, axis=0)\n",
    "chatbots_data_chunks = pd.DataFrame(data = chatbots_data_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleans the \"Message\" column\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\").replace('_',\"\")\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "chatbots_data_chunks['Message']=chatbots_data_chunks['Message'].map(lambda s:preprocess(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view chunks\n",
    "chatbots_data_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#View cleaned data, all 80,000+ rows\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "chatbots_data_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collocation finder....might scrap later\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(chatbots_data_chunks[\"Message\"].values.reshape(-1, ))\n",
    "\n",
    "#bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(2)\n",
    "\n",
    "return the 10 n-grams with the highest PMI (pointwise mutual information)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Backend portion...find the most common phrases \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range = (3,3)) \n",
    "X1 = vectorizer.fit_transform(chatbots_data_chunks[\"Message\"])  \n",
    "features = (vectorizer.get_feature_names()) \n",
    "#print(\"\\n\\nFeatures : \\n\", features) <-- if you want to see all the phrases\n",
    "#print(\"\\n\\nX1 : \\n\", X1.toarray())   <-- but I don't recommend running\n",
    "  \n",
    "# Applying TFIDF \n",
    "vectorizer = TfidfVectorizer(ngram_range = (3,3)) \n",
    "X2 = vectorizer.fit_transform(chatbots_data_chunks[\"Message\"]) \n",
    "scores = (X2.toarray()) \n",
    "#print(\"\\n\\nScores : \\n\", scores) <--- I don't recommend running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9421</th>\n",
       "      <td>der try tug</td>\n",
       "      <td>0.071186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44145</th>\n",
       "      <td>try tug juju</td>\n",
       "      <td>0.071186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44141</th>\n",
       "      <td>try try trees</td>\n",
       "      <td>0.071186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44140</th>\n",
       "      <td>try try tffff</td>\n",
       "      <td>0.071186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44139</th>\n",
       "      <td>try trees dressed</td>\n",
       "      <td>0.071186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47119</th>\n",
       "      <td>warren images nwith</td>\n",
       "      <td>0.130189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45650</th>\n",
       "      <td>virus thrives causes</td>\n",
       "      <td>0.130189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35300</th>\n",
       "      <td>seattle nursing home</td>\n",
       "      <td>0.130189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43846</th>\n",
       "      <td>transmission nhumans reservoirs</td>\n",
       "      <td>0.130189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47383</th>\n",
       "      <td>wednesday march near</td>\n",
       "      <td>0.130189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  term      rank\n",
       "9421                       der try tug  0.071186\n",
       "44145                     try tug juju  0.071186\n",
       "44141                    try try trees  0.071186\n",
       "44140                    try try tffff  0.071186\n",
       "44139                try trees dressed  0.071186\n",
       "...                                ...       ...\n",
       "47119              warren images nwith  0.130189\n",
       "45650             virus thrives causes  0.130189\n",
       "35300             seattle nursing home  0.130189\n",
       "43846  transmission nhumans reservoirs  0.130189\n",
       "47383             wednesday march near  0.130189\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Getting bottom 100 ranking features \n",
    "sums = X2.sum(axis = 0) \n",
    "data1 = [] \n",
    "for col, term in enumerate(features): \n",
    "    data1.append( (term, sums[0,col] )) \n",
    "ranking = pd.DataFrame(data1, columns = ['term','rank']) \n",
    "words = (ranking.sort_values('rank', ascending = True)) \n",
    "# Alternatively, put \"False\" here to see the most common trigrams\n",
    "\n",
    "words.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backend part that allows fuzzy to work\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "choices = chatbots_data_chunks['Message'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# much services cost\n",
    "# it appears that users are curious about the services the bots provide, as well as how much those services might cost\n",
    "# however, fuzzywuzzy is also pulling \"STIs\" for \"services\" which could be an issue\n",
    "process.extract(\"much services cost\", choices, limit = 10, scorer = fuzz.token_sort_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change mind sex\n",
    "process.extract(\"change mind sex\", choices, limit = 10, scorer = fuzz.token_sort_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#know ready sex\n",
    "process.extract(\"know ready sex\", choices, limit = 10, scorer = fuzz.token_sort_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('crubadan')\n",
    "#N-Gram based categorization, identifying languages...which is very off...\n",
    "\n",
    "tc = TextCat()\n",
    "chatbots_data_chunks[\"Message\"][0:5].apply(tc.guess_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./.cache/pip/wheels/4a/64/25/2e1cedbf7513fecda8d396a0431f3a1e217f45f1ab565e8ce8/sparse_dot_topn-0.2.9-cp38-cp38-linux_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.8/site-packages (from sparse_dot_topn) (1.18.5)\n",
      "Requirement already satisfied: scipy>=1.2.3 in /opt/conda/lib/python3.8/site-packages (from sparse_dot_topn) (1.5.2)\n",
      "Requirement already satisfied: cython>=0.29.15 in /opt/conda/lib/python3.8/site-packages (from sparse_dot_topn) (0.29.21)\n",
      "Requirement already satisfied: setuptools>=18.0 in /opt/conda/lib/python3.8/site-packages (from sparse_dot_topn) (49.6.0.post20201009)\n",
      "Installing collected packages: sparse-dot-topn\n",
      "Successfully installed sparse-dot-topn-0.2.9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "!pip install sparse_dot_topn\n",
    "import sparse_dot_topn.sparse_dot_topn as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend portion for string matching\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def ngrams(string, n=3):\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "message = chatbots_data_chunks[\"Message\"]\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(message)\n",
    "\n",
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    "    idx_dtype = np.int32\n",
    "    nnz_max = M*ntop\n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "    ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "    return csr_matrix((data,indices,indptr),shape=(M,N))\n",
    "\n",
    "import time\n",
    "t1 = time.time()\n",
    "matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.8)\n",
    "t = time.time()-t1\n",
    "\n",
    "def get_matches_df(sparse_matrix, name_vector, top=100):\n",
    "    non_zeros = sparse_matrix.nonzero()\n",
    "    sparserows = non_zeros[0]\n",
    "    sparsecols = non_zeros[1]\n",
    "    if top:\n",
    "        nr_matches = top\n",
    "    else:\n",
    "        nr_matches = sparsecols.size\n",
    "    left_side = np.empty([nr_matches], dtype=object)\n",
    "    right_side = np.empty([nr_matches], dtype=object)\n",
    "    similairity = np.zeros(nr_matches)\n",
    "    \n",
    "    for index in range(0, nr_matches):\n",
    "        left_side[index] = name_vector[sparserows[index]]\n",
    "        right_side[index] = name_vector[sparsecols[index]]\n",
    "        similairity[index] = sparse_matrix.data[index]\n",
    "    \n",
    "    return pd.DataFrame({'left_side': left_side,\n",
    "                          'right_side': right_side,\n",
    "                           'similarity': similairity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#string-matching output\n",
    "matches_df = get_matches_df(matches, message, top=100000)\n",
    "matches_df = matches_df[matches_df['similarity'] < 0.99999] # Remove all exact matches\n",
    "matches_df.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use google translate's algorithm to identify languages, tag each n-gram with their corresponding language\n",
    "# It should be easier to identify slang once google's algorithm sorts out language translations\n",
    "# Dig around for the words that are not recognized by the algorithm using some sort of metric...perhaps those will be slang?\n",
    "\n",
    "!python -m spacy download en\n",
    "!pip install googletrans\n",
    "!pip install spacy-langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "greetings = chatbots_data_chunks[chatbots_data_chunks['Message'].str.contains(\"hello|hi |big sis\")]\n",
    "greetings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply language algorithm to greetings \n",
    "\n",
    "greetings_str = greetings['Message'].astype(str).values.tolist()\n",
    "greetings_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'de', 'score': 0.9999968836486163}\n",
      "This is English text. {'language': 'en', 'score': 0.9999978023011322}\n",
      "Er lebt mit seinen Eltern und seiner Schwester in Berlin. {'language': 'de', 'score': 0.9999952516595048}\n"
     ]
    }
   ],
   "source": [
    "## Example function\n",
    "import spacy\n",
    "\n",
    "from spacy_langdetect import LanguageDetector\n",
    "nlp = spacy.load(\"en\")\n",
    "nlp.add_pipe(LanguageDetector(), name=\"language_detector\", last=True)\n",
    "text = \"This is English text. Er lebt mit seinen Eltern und seiner Schwester in Berlin.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# document level language detection\n",
    "print(doc._.language)\n",
    "# sentence level language detection\n",
    "for i, sent in enumerate(doc.sents):\n",
    "    print(sent, sent._.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join strings\n",
    "greetings_str = '. '.join(greetings_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying to apply the function above to our chatbot data after converting the dataframe to a string\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "nlp.add_pipe(LanguageDetector(), name=\"language_detector\", last=True)\n",
    "text = greetings_str\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## See results?\n",
    "\n",
    "print(doc._.language)\n",
    "\n",
    "for i, sent in enumerate(doc.sents):\n",
    "    print(sent, sent._.language)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
